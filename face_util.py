def take_photo(filename='photo.jpg', quality=0.8):
  from IPython.display import display, Javascript
  from google.colab.output import eval_js
  from base64 import b64decode
  
  
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = '사진캡쳐';
      div.appendChild(capture);
      
      const input = document.createElement('input');
      input.placeholder = '이름';
      div.appendChild(input);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return { image: canvas.toDataURL('image/jpeg', quality), label: input.value };
      }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  image = data["image"]
  label = data["label"]
  binary = b64decode(image.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename, label

def recognition(filename, known_face):
  import face_recognition 
  import cv2
  import numpy as np
  from google.colab.patches import cv2_imshow
  
  frame = face_recognition.load_image_file(filename)
  small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
  rgb_small_frame = small_frame[:, :, ::-1]

  face_locations = face_recognition.face_locations(rgb_small_frame)
  face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)

  face_names = []


  for face_encoding in face_encodings:
    matches = face_recognition.compare_faces(known_face["encodings"], face_encoding)
    name = "Unknown"

    face_distances = face_recognition.face_distance(known_face["encodings"], face_encoding)
    best_match_index = np.argmin(face_distances)
    if matches[best_match_index]:
        name = known_face["names"][best_match_index]

    face_names.append(name)

  for (top, right, bottom, left), name in zip(face_locations, face_names):
    # Scale back up face locations since the frame we detected in was scaled to 1/4 size
    top *= 4
    right *= 4
    bottom *= 4
    left *= 4

    # Draw a box around the face
    cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)

    # Draw a label with a name below the face
    cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
    font = cv2.FONT_HERSHEY_DUPLEX
    cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)
  
  cv2_imshow(frame)
